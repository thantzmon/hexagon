This program uses parcel ids and the corresponding county website to get ownership information. To run the program, go to the terminal, go to the directory "hexagon" whereever you choose to store it, and type "python grabData[County Name] PRNs.txt". grabData[County Name] must be written specifically for each county, as every website is slightly different. PRNs.txt is meant to be copied straight from a csv file, so the first line is assumed to be a header that says "Parcel ID" or something similar. Therefore the first line is always ignored.
To change this program, find the most similar website that is already programmed, and copy the code onto a new file. In this new program, change all the search values to those that correspond to the website that is being used (Ex: The owner name can be "Owner", "Account Name", "Name", or many other values). So far, every website I have looked for has a page somewhere with the parcel id somewhere in the url, so the problem lies in finding where that is.
When there is an error reading a page, I found the best way to get a bearing on what is going on is to print f.read() the line directly after creating the variable f. (Note: if you do it after the line soup = BeautifulSoup(f), f.read() will return nothing.) f.read() returns the HTML from the page that is read. After printing f.read(), you can copy the html into result.html and open the HTML file using your web browser. 
To find the exact format of the website you are scraping, I normally print all_text. The value printed is the string you search through yesterday, so if you know where the values you are looking for are, you will be able to have a better idea of what you need to do to isolate the needed value. 
The final debugging method is printing all of the variables you found individually after searching for them, and before you call return. If you see unexpected values, you can look into exactly why you got these values by referring to all_text or the website for that particular parcel id.